{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZMS_K6Pqzu1",
        "outputId": "9a3c2721-f85c-4b82-d1ca-813556a84079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.2/328.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m461.0/461.0 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q sentence-transformers langchain chromadb pypdf faiss-cpu \\\n",
        "langchain_community scikit-learn numpy mistralai langchain-mistralai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "from langchain_classic.chains import ConversationalRetrievalChain\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n"
      ],
      "metadata": {
        "id": "ZxPq0C2irHyL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"MISTRAL_API_KEY\"] = getpass(\"Enter Mistral API Key: \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8BvyRYprSTI",
        "outputId": "63428466-ad8d-471d-8c50-43c67c8c7055"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Mistral API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rgFBG7XrcI9",
        "outputId": "d2f8fa62-6c03-47b1-bc54-bccdcab4ea07"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/Day-16  Memory-Augmented-RAG.pdf\")   # upload your PDF to colab\n",
        "docs = loader.load()\n",
        "len(docs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzJ3rqb2rfAU",
        "outputId": "ee6876fa-1c0a-40fb-89aa-3d35955005d0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "len(chunks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmKN0jOor3ZG",
        "outputId": "2240b1e6-f516-4c4e-a900-363fc2b51e05"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
        "\n",
        "vector_db = FAISS.from_documents(chunks, embedding=embeddings)\n",
        "retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GpxPVn5Qr9ZR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "You are a Memory-Augmented RAG assistant.\n",
        "\n",
        "Conversation History:\n",
        "{chat_history}\n",
        "\n",
        "Relevant Retrieved Context:\n",
        "{context}\n",
        "\n",
        "User Query:\n",
        "{question}\n",
        "\n",
        "Provide a context-aware answer. ALWAYS use history if relevant.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"context\", \"question\"],\n",
        "    template=template\n",
        ")\n"
      ],
      "metadata": {
        "id": "YEk7FAK5v_HS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqOiP7Llw37D",
        "outputId": "66876dca-b458-4e2c-c13d-f59fb01714ee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3186262939.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
        "\n",
        "rag_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    combine_docs_chain_kwargs={\"prompt\": prompt}\n",
        ")\n"
      ],
      "metadata": {
        "id": "88LsXqiRwGcY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    q = input(\"You: \")\n",
        "    if q.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "    resp = rag_chain.invoke({\"question\": q})\n",
        "    print(\"Bot:\", resp[\"answer\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X9Yi0iCxZAk",
        "outputId": "c3bb4b7e-ea03-4020-dc91-e585e5d02cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: what is the document about\n",
            "Bot: Based on the **conversation history** and the **retrieved context**, the document (or image labeled *\"preencoded.png\"* in the provided architecture diagram) appears to describe the **State-Augmented Retrieval-Augmented Generation (RAG) architecture**. Here’s a breakdown of its key components and purpose:\n",
            "\n",
            "### **What the Document is About:**\n",
            "1. **Overview of State-Augmented RAG**:\n",
            "   - It outlines a system that enhances traditional RAG by incorporating **conversation memory** (state) to improve response accuracy and personalization.\n",
            "   - Unlike standard RAG, which relies only on retrieved documents, this architecture **augments the query with both historical context and external knowledge**.\n",
            "\n",
            "2. **Core Components**:\n",
            "   - **Inputs**:\n",
            "     - *User Query*: The current question/input.\n",
            "     - *Conversation History*: Past interactions to maintain continuity.\n",
            "     - *Retrieved Documents*: External knowledge sources (e.g., databases, web pages).\n",
            "   - **Conversation Memory**:\n",
            "     - Tracks the dialogue state to ensure responses are **context-aware** (e.g., remembering user preferences or prior topics).\n",
            "   - **Query & Retrieval**:\n",
            "     - Searches for relevant documents *and* past conversations to enrich the context.\n",
            "   - **Augment Context**:\n",
            "     - Combines retrieved facts with conversation history for a **comprehensive input** to the LLM.\n",
            "   - **LLM Generation**:\n",
            "     - Produces a response that is **personalized, accurate, and coherent** with the ongoing dialogue.\n",
            "   - **Output**:\n",
            "     - A **context-aware response** that flows naturally, as if continuing a conversation.\n",
            "\n",
            "3. **Key Advantages**:\n",
            "   - **Personalization**: Adapts to the user’s specific needs (e.g., tone, preferences).\n",
            "   - **Continuity**: Maintains context across multiple turns (e.g., follow-up questions).\n",
            "   - **Accuracy**: Combines historical data and retrieved facts to reduce hallucinations.\n",
            "\n",
            "### **Why This Matters in Your Query**:\n",
            "Since you asked *\"what is the document about\"* in the context of this architecture, the answer ties directly to the **diagram’s purpose**: explaining how **memory-augmented RAG** works to create more dynamic, human-like interactions.\n",
            "\n",
            "Would you like a deeper dive into any specific part (e.g., how conversation memory differs from standard RAG)?\n",
            "You: what is multiturn\n",
            "Bot: ### **Multi-Turn in State-Augmented RAG: Context-Aware Explanation**\n",
            "\n",
            "In the context of **State-Augmented Retrieval-Augmented Generation (RAG)**, **\"multi-turn\"** refers to the system’s ability to **maintain and leverage conversation history** across multiple interactions (turns) to generate **coherent, personalized, and contextually accurate responses**. Unlike single-turn RAG (where each query is treated independently), multi-turn RAG ensures continuity by:\n",
            "\n",
            "---\n",
            "\n",
            "### **Key Aspects of Multi-Turn in State-Augmented RAG**\n",
            "1. **Conversation Memory Integration**\n",
            "   - The system **tracks prior exchanges** (e.g., questions, answers, user preferences) in a structured \"state\" (e.g., a dialogue history buffer or vector store).\n",
            "   - *Example*: If you ask *\"Who is Sundar Pichai?\"* followed by *\"Where did he go to college?\"*, the system remembers \"he\" refers to Sundar Pichai, avoiding ambiguity.\n",
            "\n",
            "2. **Dynamic Query Augmentation**\n",
            "   - Each new query is **enriched with historical context** before retrieval. This ensures the LLM receives:\n",
            "     - The **current question** (e.g., *\"What’s his role now?\"*).\n",
            "     - **Past context** (e.g., *\"Sundar Pichai is CEO of Alphabet\"*).\n",
            "     - **Retrieved documents** (e.g., recent news about Alphabet’s leadership).\n",
            "   - *Why it matters*: Prevents redundant answers and enables **follow-up questions** without restarting the conversation.\n",
            "\n",
            "3. **Entity and Topic Consistency**\n",
            "   - The system **resolves references** (e.g., pronouns, implied subjects) and **maintains topic focus** across turns.\n",
            "   - *Example*:\n",
            "     - *Turn 1*: *\"Tell me about Tesla’s stock.\"*\n",
            "     - *Turn 2*: *\"How has it performed this year?\"* → \"It\" is correctly linked to Tesla’s stock.\n",
            "\n",
            "4. **Carrying Over Decisions/Preferences**\n",
            "   - User choices or constraints are **persisted** across turns.\n",
            "   - *Example*:\n",
            "     - *Turn 1*: *\"Recommend a laptop under $1000.\"*\n",
            "     - *Turn 2*: *\"Does it have a backlit keyboard?\"* → The system filters options based on the prior budget constraint.\n",
            "\n",
            "5. **Reduced Hallucinations**\n",
            "   - By grounding responses in **both retrieved documents and conversation history**, the LLM avoids contradicting earlier answers or fabricating details.\n",
            "\n",
            "---\n",
            "\n",
            "### **How State-Augmented RAG Enables Multi-Turn**\n",
            "From the architecture diagram (*preencoded.png*), here’s how it works step-by-step:\n",
            "1. **Inputs**:\n",
            "   - User query (e.g., *\"What’s his latest project?\"*).\n",
            "   - **Conversation history** (e.g., *\"Sundar Pichai is CEO of Alphabet\"*).\n",
            "   - Retrieved documents (e.g., recent Alphabet announcements).\n",
            "2. **Conversation Memory**:\n",
            "   - Stores and retrieves past turns to **disambiguate pronouns** (\"his\") and **maintain context**.\n",
            "3. **Augmented Context**:\n",
            "   - Combines the query, history, and documents into a **unified prompt** for the LLM.\n",
            "4. **LLM Generation**:\n",
            "   - Produces a response that **acknowledges prior turns** (e.g., *\"Sundar Pichai’s latest project at Alphabet is...\"*).\n",
            "\n",
            "---\n",
            "\n",
            "### **Real-World Example**\n",
            "| **Turn** | **User Query**               | **State-Augmented RAG Response**                                                                 |\n",
            "|----------|------------------------------|--------------------------------------------------------------------------------------------------|\n",
            "| 1        | \"Who is Sundar Pichai?\"      | *\"Sundar Pichai is the CEO of Alphabet (Google’s parent company). He joined Google in 2004.\"*    |\n",
            "| 2        | \"Where did he go to college?\"| *\"Sundar Pichai earned his degree from the Indian Institute of Technology (IIT) Kharagpur.\"*     |\n",
            "| 3        | \"What’s his latest project?\" | *\"As of 2024, Sundar Pichai is focusing on AI advancements at Alphabet, including Gemini.\"*      |\n",
            "\n",
            "---\n",
            "\n",
            "### **Why This Differs from Standard RAG**\n",
            "| **Feature**               | **Standard RAG**                          | **State-Augmented RAG (Multi-Turn)**                     |\n",
            "|---------------------------|-------------------------------------------|----------------------------------------------------------|\n",
            "| **Context Handling**      | Treats each query independently.          | Uses conversation history to link turns.                 |\n",
            "| **Follow-Up Questions**   | May fail to resolve pronouns (e.g., \"it\").| Correctly links \"it\" to prior entities.                  |\n",
            "| **Personalization**       | Generic responses.                        | Adapts to user preferences (e.g., tone, prior choices).  |\n",
            "| **Hallucination Risk**    | Higher (no memory of past answers).       | Lower (grounded in history + documents).                 |\n",
            "\n",
            "---\n",
            "\n",
            "### **When Multi-Turn Matters Most**\n",
            "- **Complex Tasks**: E.g., troubleshooting, research, or planning (where each step builds on the last).\n",
            "- **Personalized Assistants**: E.g., healthcare bots remembering patient history or shopping assistants recalling preferences.\n",
            "- **Long-Form Dialogues**: E.g., customer support, tutoring, or creative brainstorming.\n",
            "\n",
            "Would you like to explore how this compares to other memory-augmented systems (e.g., chatbots with short-term vs. long-term memory)?\n"
          ]
        }
      ]
    }
  ]
}